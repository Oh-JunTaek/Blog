
# Human Pose Estimation

![](https://blog.kakaocdn.net/dna/bSCvu9/btsMa41TcZ3/AAAAAAAAAAAAAAAAAAAAAF-vy04lv0VD2pAYy-nxEJQ0nBu95bXxukJy1isyNMYX/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=4RnVbUXRpK6XR0pMyju%2BXIRYeaY%3D)

HPEê°€ ë¬´ì—‡ì¸ì§€ ê¶ê¸ˆí•´ì„œ ì§ì ‘ ê³µë¶€í•´ë³´ê³  ì§€ì›í•˜ê¸°ë¡œ í–ˆë‹¤!!

ê³„íšì„œ : [https://eunmastudio.tistory.com/46](https://eunmastudio.tistory.com/46)  
ê¹ƒí—ˆë¸Œ : [https://github.com/Oh-JunTaek/sideProject/tree/main/Human%20Pose%20Estimation](https://github.com/Oh-JunTaek/sideProject/tree/main/Human%20Pose%20Estimation)

* * *

ì¤€ë¹„ë¬¼
---

ë°ì´í„° ì…‹ ì„ ì • : COCOÂ 

*   **ëŒ€ê·œëª¨ ë°ì´í„°ì™€ ë‹¤ì–‘ì„±:**  
    COCO ë°ì´í„°ì…‹ì€ ìˆ˜ì‹­ë§Œ ì¥ì˜ ì´ë¯¸ì§€ì™€ ë‹¤ì–‘í•œ í™˜ê²½, í¬ì¦ˆ, ì¸ë¬¼ êµ¬ì„±ì„ í¬í•¨í•˜ê³  ìˆì–´, ì‹¤ì œ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ìƒí™©ì„ ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤.
*   **í’ë¶€í•œ ì–´ë…¸í…Œì´ì…˜:**  
    ì‚¬ëŒì˜ keypoints(ê´€ì ˆ) ì–´ë…¸í…Œì´ì…˜ì´ ì •ë°€í•˜ê²Œ ì œê³µë˜ë©°, ê°ì²´ ê²€ì¶œ, ì„¸ë¶„í™” ë“± ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë¥¼ ìœ„í•œ ì–´ë…¸í…Œì´ì…˜ì´ í•¨ê»˜ í¬í•¨ë˜ì–´ ìˆì–´, ì—¬ëŸ¬ ì—°êµ¬ ë° ì‘ìš© ë¶„ì•¼ì—ì„œ í™œìš©í•˜ê¸° ì¢‹ìŠµë‹ˆë‹¤.
*   **í‘œì¤€ ë²¤ì¹˜ë§ˆí¬:**  
    COCO ë°ì´í„°ì…‹ì€ ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” í‘œì¤€ ë²¤ì¹˜ë§ˆí¬ë¡œ, ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° í‰ê°€ì— ìˆì–´ ê¸°ì¤€ ì—­í• ì„ í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì—°êµ¬ ê²°ê³¼ë¥¼ ë‹¤ë¥¸ ì—°êµ¬ì™€ ë¹„êµí•˜ê±°ë‚˜, ê¸°ì¡´ ëª¨ë¸ê³¼ì˜ ì„±ëŠ¥ ì°¨ì´ë¥¼ ëª…í™•íˆ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
*   **í™œë°œí•œ ì»¤ë®¤ë‹ˆí‹° ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì§€ì›:**  
    COCO ë°ì´í„°ì…‹ì€ pycocotoolsì™€ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ê´€ë ¨ ë¬¸ì„œì™€ íŠœí† ë¦¬ì–¼, ì˜¤í”ˆ ì†ŒìŠ¤ í”„ë¡œì íŠ¸ë“¤ì´ ë§ì•„ ì´ˆê¸° í•™ìŠµ ë° êµ¬í˜„ì— í° ë„ì›€ì´ ë©ë‹ˆë‹¤.

* * *

![](https://blog.kakaocdn.net/dna/QmXOw/btsL88p6WYw/AAAAAAAAAAAAAAAAAAAAAHwFZVDOpTxyQHA5Uoh5dU836v_Y10565xd4woIzgeEg/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=x0hyEBPybhvHYwaLkMNnbFubMao%3D)

ê·¸ëŸ¬ë‚˜ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œê°€ ì‰½ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤... ê²€ì‚¬ ê²°ê³¼ httpì™€ httpsì‚¬ì´ì˜ ë³´ì•ˆ ë¬¸ì œì¸ ë“¯ í•©ë‹ˆë‹¤.  
ê·¸ë˜ì„œ ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ ì§ì ‘ ë³€ê²½í•´ì„œ ì„¤ì¹˜ë¥¼ ì‹œì‘! : [https://images.cocodataset.org/zips/train2017.zip](https://images.cocodataset.org/zips/train2017.zip)

![](https://blog.kakaocdn.net/dna/bmvCLK/btsL7dNulyE/AAAAAAAAAAAAAAAAAAAAAM3DcaFfCoB4n4SNzdBbqUdAJH-nnvC5leRvs4-4wdmZ/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=zr0203opGh363km1S1g6oaDLVSE%3D)

ë§ˆì°¬ê°€ì§€ë¡œ ê²€ì¦ ë°ì´í„°ì…‹ë„ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì„¤ì¹˜ : [https://images.cocodataset.org/zips/val2017.zip](http://images.cocodataset.org/zips/val2017.zip)

* * *

### ëª¨ë¸ ì•„í‚¤í…ì²˜ ì„¤ê³„ ë° êµ¬í˜„

*   **ì…ë ¥/ì¶œë ¥ ì •ì˜:**
    *   **ì…ë ¥:** ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ (ì˜ˆ: 256Ã—256 ë˜ëŠ” 368Ã—368 ë“±)
    *   **ì¶œë ¥:** ê° ê´€ì ˆ(í‚¤í¬ì¸íŠ¸)ì— í•´ë‹¹í•˜ëŠ” heatmap (ì˜ˆ: 17ê°œì˜ heatmap, ê° heatmapì€ í•´ë‹¹ ê´€ì ˆì˜ ì¡´ì¬ í™•ë¥ ì„ ë‚˜íƒ€ëƒ„)
*   **êµ¬í˜„ ë¼ì´ë¸ŒëŸ¬ë¦¬:**
    *   **TensorFlow/Keras** ë˜ëŠ” **PyTorch** ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
    *   ì„ íƒí•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ë§ì¶° ëª¨ë¸ êµ¬ì¡°(ì˜ˆ: ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´, ì—…ìƒ˜í”Œë§ ë ˆì´ì–´ ë“±)ë¥¼ ì„¤ê³„í•©ë‹ˆë‹¤.
*   **CNN ê¸°ë°˜ Heatmap Regression ëª¨ë¸ (PyTorch)**
    

    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    
    # ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜: SimplePoseNet
    class SimplePoseNet(nn.Module):
        def __init__(self, num_keypoints=17):
            super(SimplePoseNet, self).__init__()
            # Feature extractor: ì…ë ¥ ì´ë¯¸ì§€ì—ì„œ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.
            # 1) ì²« ë²ˆì§¸ Convolution: ì±„ë„ ìˆ˜ 3 (RGB ì´ë¯¸ì§€) -> 64, ì»¤ë„ í¬ê¸° 7, stride 2, padding 3
            self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)
            self.bn1 = nn.BatchNorm2d(64)
            # 2) ë‘ ë²ˆì§¸ Convolution: 64 -> 128, ì»¤ë„ í¬ê¸° 3, stride 2, padding 1
            self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)
            self.bn2 = nn.BatchNorm2d(128)
            # 3) ì„¸ ë²ˆì§¸ Convolution: 128 -> 256, ì»¤ë„ í¬ê¸° 3, stride 2, padding 1
            self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)
            self.bn3 = nn.BatchNorm2d(256)
            # ì—…ìƒ˜í”Œë§(Deconvolution): 256 ì±„ë„ì—ì„œ num_keypoints (ê¸°ë³¸ 17) ì±„ë„ì˜ heatmap ìƒì„±
            self.deconv = nn.ConvTranspose2d(256, num_keypoints, kernel_size=4, stride=2, padding=1)
    
        def forward(self, x):
            # ì…ë ¥ xì— ëŒ€í•´ ê° ì¸µì„ ìˆœì„œëŒ€ë¡œ í†µê³¼ì‹œí‚µë‹ˆë‹¤.
            x = F.relu(self.bn1(self.conv1(x)))
            x = F.relu(self.bn2(self.conv2(x)))
            x = F.relu(self.bn3(self.conv3(x)))
            # ë§ˆì§€ë§‰ì— deconvolutionì„ ì ìš©í•˜ì—¬ heatmap ìƒì„±
            heatmaps = self.deconv(x)
            return heatmaps
    
    # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì¶œë ¥
    if __name__ == "__main__":
        model = SimplePoseNet(num_keypoints=17)
        print(model)

* * *

### ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜

*   **ì´ë¯¸ì§€ íŒŒì¼ì„ ì—´ê³  RGBë¡œ ë³€í™˜**í•œ í›„,
*   **ë¦¬ì‚¬ì´ì¦ˆ**í•˜ì—¬ ì§€ì •ëœ í¬ê¸°ë¡œ ë§ì¶”ê³ ,
*   **í…ì„œë¡œ ë³€í™˜**í•œ ë’¤,
*   **ì •ê·œí™”**ë¥¼ í†µí•´ í”½ì…€ ê°’ì˜ ë¶„í¬ë¥¼ ì¡°ì •í•˜ì—¬,
*   **ëª¨ë¸ í•™ìŠµì— ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” PyTorch í…ì„œ**ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
*   **CNN ê¸°ë°˜ Heatmap Regression ëª¨ë¸ (PyTorch)**
    

    from PIL import Image
    import torchvision.transforms as transforms
    
    def preprocess_image_pil(image_path, target_size=(256, 256)):
        """
        ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ë°›ì•„ target_size í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆí•˜ê³ , í…ì„œ ë³€í™˜ ë° ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            image_path (str): ì´ë¯¸ì§€ íŒŒì¼ì˜ ê²½ë¡œ.
            target_size (tuple): (width, height) í˜•íƒœì˜ ì›í•˜ëŠ” ì¶œë ¥ í¬ê¸°.
            
        Returns:
            torch.Tensor: ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ í…ì„œ.
        """
        # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì •ì˜ (ì—¬ê¸°ì„œëŠ” ì¼ë°˜ì ì¸ ImageNet ì •ê·œí™” ê°’ ì‚¬ìš©)
        transform = transforms.Compose([
            transforms.Resize(target_size),
            transforms.ToTensor(),  # 0~1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë³€í™˜ ë° (C, H, W) í…ì„œ í˜•ì‹
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
        ])
        
        # PIL Imageë¡œ ì½ì–´ì˜¤ê¸°
        image = Image.open(image_path).convert("RGB")
        image = transform(image)
        return image

* * *

#### **ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (CustomImageDataset) ì •ì˜**

*   **ëª©ì :**  
    ì§€ì •ëœ í´ë”ì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ë“¤ì„ ì½ì–´ì˜¤ê³ , ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í…ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
*   **ì„¸ë¶€ ê¸°ëŠ¥:**
    *   \_\_init\_\_:
        *   ì „ë‹¬ë°›ì€ image\_dir í´ë” ë‚´ì—ì„œ í™•ì¥ìê°€ .jpgì¸ ëª¨ë“  íŒŒì¼ëª…ì„ ëª©ë¡ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        *   transform\_func ì¸ìë¥¼ í†µí•´, ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜(ì˜ˆ: preprocess\_image\_pil)ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    *   \_\_len\_\_:
        *   ë°ì´í„°ì…‹ì— í¬í•¨ëœ ì´ë¯¸ì§€ íŒŒì¼ì˜ ì´ ê°œìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    *   \_\_getitem\_\_:
        *   ì£¼ì–´ì§„ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œë¥¼ êµ¬ì„±í•˜ê³ , ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ í†µí•´ í•´ë‹¹ ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜í•œ í›„ ë°˜í™˜í•©ë‹ˆë‹¤.
        *   (ì¶”í›„) ì–´ë…¸í…Œì´ì…˜ ì •ë³´ë„ í•¨ê»˜ ë°˜í™˜í•˜ë„ë¡ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    from torch.utils.data import Dataset
    import os
    
    class CustomImageDataset(Dataset):
        def __init__(self, image_dir, transform_func):
            """
            Args:
                image_dir (str): ì´ë¯¸ì§€ë“¤ì´ ì €ì¥ëœ í´ë” ê²½ë¡œ.
                transform_func (callable): ì´ë¯¸ì§€ë¥¼ ì „ì²˜ë¦¬í•  í•¨ìˆ˜ (ì˜ˆ: preprocess_image_pil).
            """
            self.image_dir = image_dir
            self.transform_func = transform_func
            # ì´ë¯¸ì§€ íŒŒì¼ëª… ëª©ë¡ì„ ìˆ˜ì§‘ (í™•ì¥ìê°€ jpgì¸ íŒŒì¼ë“¤ë§Œ)
            self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]
    
        def __len__(self):
            return len(self.image_files)
    
        def __getitem__(self, idx):
            image_path = os.path.join(self.image_dir, self.image_files[idx])
            # ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ í†µí•´ ì´ë¯¸ì§€ í…ì„œë¥¼ ì–»ìŒ
            image_tensor = self.transform_func(image_path)
            # (ì¶”í›„) ì–´ë…¸í…Œì´ì…˜ ì •ë³´ë„ í•¨ê»˜ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì • ê°€ëŠ¥
            return image_tensor

* * *

### DataLoader êµ¬ì„±

**ëª©ì **  
CustomImageDataset í´ë˜ìŠ¤ë¥¼ í†µí•´ ìƒì„±í•œ ë°ì´í„°ì…‹ì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¡œë“œí•˜ì—¬, ëª¨ë¸ì— ì…ë ¥í•  ìˆ˜ ìˆë„ë¡ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.

**ì‘ì—… ë‚´ìš©:**

*   torch.utils.data.DataLoaderë¥¼ ì´ìš©í•´ ë°ì´í„°ì…‹ì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì½ì–´ì˜µë‹ˆë‹¤.
*   ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ì˜ shapeì´ë‚˜ ë‚´ìš©ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.

    from torch.utils.data import DataLoader
    from dataset import CustomImageDataset
    from preprocessing import preprocess_image_pil
    
    def create_dataloader(image_dir, batch_size=4, shuffle=True, num_workers=0):
        """
        ì§€ì •ëœ ì´ë¯¸ì§€ í´ë”ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” DataLoaderë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            image_dir (str): ì´ë¯¸ì§€ë“¤ì´ ì €ì¥ëœ í´ë” ê²½ë¡œ.
            batch_size (int): ë°°ì¹˜ í¬ê¸°.
            shuffle (bool): ë°ì´í„° ì…”í”Œ ì—¬ë¶€.
            num_workers (int): ë°ì´í„°ë¥¼ ë¡œë“œí•  ë•Œ ì‚¬ìš©í•  ì„œë¸Œ í”„ë¡œì„¸ìŠ¤ì˜ ìˆ˜.
            
        Returns:
            DataLoader: êµ¬ì„±ëœ PyTorch DataLoader ê°ì²´.
        """
        dataset = CustomImageDataset(
            image_dir=image_dir,
            transform_func=preprocess_image_pil
        )
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)
        return dataloader

* * *

pycocotoolsë¥¼ ì´ìš©í•œ í‚¤í¬ì¸íŠ¸ ì–´ë…¸í…Œì´ì…˜ ë¡œë“œ íë¦„
---------------------------------

1.  **pycocotools ì„¤ì¹˜**  
    COCO ì–´ë…¸í…Œì´ì…˜(.json)ì„ ì‰½ê²Œ íŒŒì‹±í•˜ê¸° ìœ„í•´ pycocotools ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
2.  **ì–´ë…¸í…Œì´ì…˜ íŒŒì¼ ë¡œë“œ**
    *   person\_keypoints\_train2017.json (í›ˆë ¨ìš©)
    *   person\_keypoints\_val2017.json (ê²€ì¦ìš©)  
        ì„ Python ì½”ë“œì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤.
3.  **ë°ì´í„°ì…‹ í´ë˜ìŠ¤ êµ¬ì„±**
    *   ê° ì´ë¯¸ì§€ì— ëŒ€í•œ í‚¤í¬ì¸íŠ¸(ê´€ì ˆ) ì¢Œí‘œë¥¼ ë¶ˆëŸ¬ì™€ì„œ, ì´ë¥¼ "í›ˆë ¨ì— ì‚¬ìš©í•  ì •ë‹µ(ground truth)" í˜•íƒœë¡œ ê°€ê³µí•©ë‹ˆë‹¤.
    *   ì´ë¯¸ì§€ëŠ” ê¸°ì¡´ì²˜ëŸ¼ preprocess\_image\_pil ë“±ì„ ì´ìš©í•´ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
4.  **(ì„ íƒ) Heatmap ìƒì„± ë¡œì§**
    *   Human Pose Estimationì—ì„œëŠ” ê° ê´€ì ˆ ìœ„ì¹˜ì— ëŒ€í•œ Heatmapì„ ë§Œë“¤ì–´ í•™ìŠµí•˜ëŠ” ë°©ì‹ì„ ìì£¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    *   ë§Œì•½ ì§ì ‘ CNNì„ êµ¬ì„±í•˜ì—¬ Heatmap Regression ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•˜ë ¤ë©´, **ê° ê´€ì ˆë§ˆë‹¤ 2D ê°€ìš°ì‹œì•ˆ ë¶„í¬** ë“±ì„ ê·¸ë ¤ì„œ ì§€ë„(heatmap)ë¥¼ ë§Œë“œëŠ” ë¡œì§ì´ í•„ìš”í•©ë‹ˆë‹¤.
5.  **DataLoaderë¡œ ë¬¶ì–´ì„œ ëª¨ë¸ í•™ìŠµ ë‹¨ê³„ë¡œ ì—°ê²°**
    *   ìµœì¢…ì ìœ¼ë¡œ torch.utils.data.DataLoaderì— ë°ì´í„°ì…‹ì„ ë„£ì–´ batch ë‹¨ìœ„ë¡œ ë¡œë“œí•˜ê³ , ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    from pycocotools.coco import COCO
    import os
    
    # ì˜ˆ: COCO 2017 train ì–´ë…¸í…Œì´ì…˜ JSON íŒŒì¼
    train_annotation_file = r"C:\Users\dev\Documents\GitHub\sideProject\Human Pose Estimation\data\annotations\person_keypoints_train2017.json"
    coco_train = COCO(train_annotation_file)
    
    # ì´ë¯¸ì§€ ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    img_ids = coco_train.getImgIds()
    
    # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ì •ë³´ ë¡œë“œ
    img_info = coco_train.loadImgs(img_ids[0])[0]
    print("ì´ë¯¸ì§€ íŒŒì¼ëª…:", img_info['file_name'])
    print("ì´ë¯¸ì§€ ID:", img_info['id'])
    
    # í•´ë‹¹ ì´ë¯¸ì§€ì˜ ì–´ë…¸í…Œì´ì…˜(= ì‚¬ëŒ í‚¤í¬ì¸íŠ¸ ì •ë³´ ë“±) ë¡œë“œ
    ann_ids = coco_train.getAnnIds(imgIds=img_info['id'], iscrowd=False)
    anns = coco_train.loadAnns(ann_ids)
    print("í‚¤í¬ì¸íŠ¸ ì •ë³´ ì˜ˆì‹œ:", anns)

* * *

â€œì´ë¯¸ì§€ + í‚¤í¬ì¸íŠ¸â€ë¥¼ í•©ì³ì„œ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•˜ê¸°
-----------------------------

ì—¬ê¸°ê¹Œì§€ëŠ” **ì´ë¯¸ì§€ ìì²´**ë¥¼ ë¡œë”©í•˜ê³  ì „ì²˜ë¦¬í•˜ëŠ” ê³¼ì •ê³¼, **í‚¤í¬ì¸íŠ¸ JSON**ìœ¼ë¡œë¶€í„° ì›í•˜ëŠ” ì •ë³´ë¥¼ ì–»ëŠ” ê³¼ì •ì„ **ë³„ë„ë¡œ** í•´ë´¤ìŠµë‹ˆë‹¤. ì‹¤ì œ Human Pose Estimation ëª¨ë¸ì„ í•™ìŠµí•˜ë ¤ë©´, ë‘˜ì„ í•˜ë‚˜ì˜ **Dataset** ì•ˆì—ì„œ ê²°í•©\*\*í•´ì¤˜ì•¼ í•©ë‹ˆë‹¤.

1.  **COCO Pose Dataset ì‘ì„± (ì´ë¯¸ì§€ + í‚¤í¬ì¸íŠ¸)**
    *   ìƒˆë¡œìš´ Dataset í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ì–´, COCO ê°ì²´(pycocotools.coco.COCO)ì™€ ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ë¡œë”©í•©ë‹ˆë‹¤.
    *   í•œ ì´ë¯¸ì§€ì— ì—¬ëŸ¬ ì‚¬ëŒ(í‚¤í¬ì¸íŠ¸)ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë‹¨ìˆœíˆ ì²« ë²ˆì§¸ ì‚¬ëŒë§Œ ì‚¬ìš©í•˜ê±°ë‚˜, ë©€í‹° í¼ìŠ¨ ì²˜ë¦¬ë¥¼ ì§€ì›í•˜ë„ë¡ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    *   **Heatmap ìƒì„±**ì„ ì›í•œë‹¤ë©´, í‚¤í¬ì¸íŠ¸ ì¢Œí‘œë¥¼ Heatmap í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ë¡œì§ì„ ë„£ìŠµë‹ˆë‹¤. (ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬, ì› ê·¸ë¦¬ê¸° ë“±)

    import os
    from torch.utils.data import Dataset
    from pycocotools.coco import COCO
    from PIL import Image
    import torchvision.transforms as transforms
    import torch
    import numpy as np
    import cv2  # Heatmap ê·¸ë¦´ ë•Œ ìœ ìš©
    
    class COCOPoseDataset(Dataset):
        def __init__(self, img_dir, ann_file, input_size=(256, 256)):
            self.img_dir = img_dir
            self.coco = COCO(ann_file)
            self.img_ids = self.coco.getImgIds()
            self.input_size = input_size
    
            # ì˜ˆì‹œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
            self.transform = transforms.Compose([
                transforms.Resize(self.input_size),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406],
                                     [0.229, 0.224, 0.225])
            ])
    
        def __len__(self):
            return len(self.img_ids)
    
        def __getitem__(self, idx):
            img_id = self.img_ids[idx]
            img_info = self.coco.loadImgs(img_id)[0]
            image_path = os.path.join(self.img_dir, img_info['file_name'])
    
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = Image.open(image_path).convert('RGB')
    
            # í‚¤í¬ì¸íŠ¸ ë¡œë“œ
            ann_ids = self.coco.getAnnIds(imgIds=img_id, iscrowd=False)
            anns = self.coco.loadAnns(ann_ids)
    
            # (ê°„ë‹¨ ì˜ˆì‹œ) ì²« ë²ˆì§¸ ì‚¬ëŒë§Œ í‚¤í¬ì¸íŠ¸ ì‚¬ìš©
            if len(anns) > 0 and 'keypoints' in anns[0]:
                keypoints = anns[0]['keypoints']
            else:
                # ì‚¬ëŒì´ ì—†ê±°ë‚˜ í‚¤í¬ì¸íŠ¸ê°€ ì—†ëŠ” ê²½ìš°
                keypoints = [0]*(17*3)  # 17ê°œ ê´€ì ˆ * (x,y,v)
    
            # Heatmap ìƒì„±(ì˜µì…˜): ëª¨ë¸ì´ Heatmap Regression ë°©ì‹ì„ ì“¸ ê²½ìš°
            # ê°„ë‹¨íˆ 64x64 í¬ê¸°ë¡œ ì¤„ì—¬ì„œ ê´€ì ˆ ìœ„ì¹˜ ì°ê¸° ì˜ˆì‹œ
            heatmap_size = (self.input_size[1]//4, self.input_size[0]//4)
            num_keypoints = 17
            heatmaps = np.zeros((num_keypoints, heatmap_size[0], heatmap_size[1]), dtype=np.float32)
    
            # ê°€ìš°ì‹œì•ˆ ëŒ€ì‹  ì›(circle)ìœ¼ë¡œ í‘œì‹œ (ì˜ˆì‹œ)
            for kp in range(num_keypoints):
                x = keypoints[kp*3+0]
                y = keypoints[kp*3+1]
                v = keypoints[kp*3+2]
                if v > 0:  # ë³´ì´ëŠ” ê´€ì ˆì—ë§Œ
                    # Heatmap ì¢Œí‘œë¡œ ìŠ¤ì¼€ì¼
                    x_hm = int(x * heatmap_size[1]/img_info['width'])
                    y_hm = int(y * heatmap_size[0]/img_info['height'])
                    if 0 <= x_hm < heatmap_size[1] and 0 <= y_hm < heatmap_size[0]:
                        cv2.circle(heatmaps[kp], (x_hm, y_hm), 2, 1.0, -1)
    
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            image_tensor = self.transform(image)
            heatmaps_tensor = torch.from_numpy(heatmaps)
    
            return image_tensor, heatmaps_tensor

* * *

ë‹¤ìŒ ë‹¨ê³„
-----

**ê°„ë‹¨í•œ í•™ìŠµ ë£¨í”„ ì‘ì„±**

    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader
    from coco_pose_dataset import COCOPoseDataset
    from simple_pose_net import SimplePoseNet
    
    def train_pose_model():
        # 1) í•˜ë“œì›¨ì–´ ë””ë°”ì´ìŠ¤ ì„¤ì • (GPU ì‚¬ìš© ê°€ëŠ¥ì‹œ cuda)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print("Using device:", device)
    
        # 2) Dataset & DataLoader êµ¬ì„±
        img_dir = r"C:\Users\dev\Documents\GitHub\sideProject\Human Pose Estimation\data\train2017"
        ann_file = r"C:\Users\dev\Documents\GitHub\sideProject\Human Pose Estimation\data\annotations\person_keypoints_train2017.json"
        train_dataset = COCOPoseDataset(img_dir, ann_file)
        train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
    
        # 3) ëª¨ë¸ ì¤€ë¹„
        model = SimplePoseNet(num_keypoints=17).to(device)
    
        # 4) ì†ì‹¤ í•¨ìˆ˜ & ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=1e-3)
    
        # 5) í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •
        num_epochs = 5
    
        # 6) í•™ìŠµ ë£¨í”„
        for epoch in range(num_epochs):
            model.train()  # í•™ìŠµ ëª¨ë“œ
            running_loss = 0.0
    
            for batch_idx, (images, heatmaps) in enumerate(train_loader):
                # ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (GPU ì‚¬ìš© ì‹œ)
                images = images.to(device)
                heatmaps = heatmaps.to(device)
    
                # forward
                outputs = model(images)
                loss = criterion(outputs, heatmaps)
    
                # backward
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
    
                # ë¡œê·¸ ê¸°ë¡
                running_loss += loss.item()
    
                if (batch_idx + 1) % 50 == 0:
                    print(f"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], "
                          f"Loss: {loss.item():.4f}")
    
            # ì—í¬í¬ë³„ í‰ê·  Loss ì¶œë ¥
            epoch_loss = running_loss / len(train_loader)
            print(f"==> Epoch [{epoch+1}/{num_epochs}] Mean Loss: {epoch_loss:.4f}")
    
        print("Training finished.")
    
    if __name__ == "__main__":
        train_pose_model()

*   **ë””ë°”ì´ìŠ¤ ì„¤ì •**
    *   GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPUë¥¼ ì‚¬ìš©í•˜ê³ , ì•„ë‹ˆë©´ CPUë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
*   python
    
    ë³µì‚¬í¸ì§‘
    
    device = torch.device("cuda" if torch.cuda.is\_available() else "cpu") model.to(device)
    
*   **Dataset & DataLoader**
    *   ì´ì „ì— ë§Œë“¤ì—ˆë˜ COCOPoseDatasetì„ ì‚¬ìš©í•˜ì—¬, í•œ ë²ˆì— **(ì´ë¯¸ì§€, heatmaps)** ë°°ì¹˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
*   **ì†ì‹¤ í•¨ìˆ˜ & ì˜µí‹°ë§ˆì´ì €**
    *   **MSELoss**: ëª¨ë¸ì˜ ì˜ˆì¸¡ Heatmapê³¼ ì •ë‹µ Heatmap ê°„ì˜ Mean Squared Errorë¥¼ ê³„ì‚°.
    *   **Adam**: ë¹„êµì  ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµ ê°€ëŠ¥í•œ ì˜µí‹°ë§ˆì´ì €.
*   **í•™ìŠµ ë£¨í”„ êµ¬ì¡°**
    *   ì—í¬í¬(num\_epochs)ë§Œí¼ ë°˜ë³µí•˜ë©´ì„œ, ê° ë°°ì¹˜(batch\_idx)ë§ˆë‹¤ ë‹¤ìŒì„ ìˆ˜í–‰:
        1.  **forward pass**: outputs = model(images)
        2.  **loss ê³„ì‚°**: loss = criterion(outputs, heatmaps)
        3.  **ì—­ì „íŒŒ(gradient backprop)**: loss.backward()
        4.  **ì˜µí‹°ë§ˆì´ì € ì—…ë°ì´íŠ¸**: optimizer.step()
    *   running\_lossì— ë°°ì¹˜ë³„ lossë¥¼ ëˆ„ì í•˜ì—¬, **ì—í¬í¬ê°€ ëë‚œ í›„ í‰ê·  Loss**ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
*   **ë¡œê·¸ / ëª¨ë‹ˆí„°ë§**
    *   (batch\_idx + 1)ì´ 50 ë°°ìˆ˜ì¼ ë•Œë§ˆë‹¤ ì¤‘ê°„ Lossë¥¼ ì¶œë ¥í•˜ë„ë¡ ì˜ˆì‹œ ì„¤ì •
    *   ì—í¬í¬ê°€ ëë‚˜ë©´ í•´ë‹¹ ì—í¬í¬ì˜ í‰ê·  Loss(epoch\_loss)ë¥¼ í‘œì‹œ

* * *

cudaë¥¼ ì¸ì‹í•˜ì§€ ëª»í•´ì„œ!!!

![](https://blog.kakaocdn.net/dna/bEh1xM/btsL9RvQJSg/AAAAAAAAAAAAAAAAAAAAAIB_YGcoY-Ia6DashhjjDbHZtiB4ywOOWOyC6lV-KW1l/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=fk1Khd4BMLtGiojLDfLVOOHtlwY%3D)

cudaì„¤ì¹˜ë¥¼ í•˜ê³  ì™”ìŠµë‹ˆë‹¤.

![](https://blog.kakaocdn.net/dna/sytXK/btsMavlmiTW/AAAAAAAAAAAAAAAAAAAAAAJ7pXVUvTWIkNdO6u9HQ8RKOXvtcrfJxhrf4qHqtaNo/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=3TcpY0SDUL25CfpgxnhEXW9Ii0I%3D)

ì„±ê³µì ìœ¼ë¡œ ê·¸ë˜í”½ì¹´ë“œë¥¼ ê´´ë¡­íˆê¸° ì‹œì‘í–ˆìŠµë‹ˆë‹¤.

* * *

**ëª¨ë¸ì„ ì‚¬ìš©í•´ ìƒˆë¡œìš´ ì´ë¯¸ì§€ì—ì„œ í‚¤í¬ì¸íŠ¸ ì˜ˆì¸¡**
-----------------------------

í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•´ì„œ ìƒˆë¡œìš´ ì´ë¯¸ì§€ë¥¼ ì…ë ¥í•˜ê³  í‚¤í¬ì¸íŠ¸ë¥¼ ì˜ˆì¸¡í•˜ëŠ” **Inference ì½”ë“œ**ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.

    import torch
    import numpy as np
    import cv2
    from PIL import Image
    import torchvision.transforms as transforms
    from simple_pose_net import SimplePoseNet
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    model = SimplePoseNet(num_keypoints=17).to(device)
    model.load_state_dict(torch.load("pose_model.pth", map_location=device))
    model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    
    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜
    def preprocess_image(image_path, target_size=(256, 256)):
        transform = transforms.Compose([
            transforms.Resize(target_size),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        image = Image.open(image_path).convert("RGB")
        image = transform(image).unsqueeze(0).to(device)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ í›„ GPUë¡œ ì´ë™
        return image
    
    # ì˜ˆì¸¡ ìˆ˜í–‰
    def predict_pose(image_path):
        image = preprocess_image(image_path)
        
        with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
            output = model(image)  # ëª¨ë¸ ì‹¤í–‰ (ì¶œë ¥ì€ (1, 17, 64, 64) í˜•íƒœì˜ Heatmap)
    
        return output.cpu().numpy()
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì— ëŒ€í•´ ì‹¤í–‰
    image_path = "path/to/your/image.jpg"  # ì˜ˆì¸¡í•  ì‹¤ì œ ì´ë¯¸ì§€ ê²½ë¡œ ì…ë ¥
    heatmaps = predict_pose(image_path)
    
    print("âœ… Inference completed! Heatmap shape:", heatmaps.shape)

![](https://blog.kakaocdn.net/dna/DHzWD/btsL8YoXaAD/AAAAAAAAAAAAAAAAAAAAAGijo-ZMmKHdKk6u45xKrfgc6NvtcSj8jDEXlDEkzjXf/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=bQ2pD0JJpzjKQb6Cyzic5346RYQ%3D)

* * *

**ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”**
-------------

í•™ìŠµëœ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ **í‚¤í¬ì¸íŠ¸ë¥¼ ì‹¤ì œ ì´ë¯¸ì§€ ìœ„ì— ì‹œê°ì ìœ¼ë¡œ í‘œí˜„**í•˜ë©´ ì§ê´€ì ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    import cv2
    import numpy as np
    import torch
    from inference import predict_pose, preprocess_image
    from PIL import Image
    
    # í‚¤í¬ì¸íŠ¸ë¥¼ ì›ë³¸ ì´ë¯¸ì§€ ìœ„ì— ê·¸ë¦¬ëŠ” í•¨ìˆ˜
    def draw_keypoints(image_path, heatmaps):
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # OpenCVëŠ” BGR, PILì€ RGBì´ë¯€ë¡œ ë³€í™˜
    
        num_keypoints = heatmaps.shape[1]
        heatmap_size = heatmaps.shape[2]
    
        # ì´ë¯¸ì§€ í¬ê¸°ì— ë§ê²Œ í‚¤í¬ì¸íŠ¸ ì¢Œí‘œ ë³€í™˜
        height, width, _ = image.shape
        scale_x = width / heatmap_size
        scale_y = height / heatmap_size
    
        for kp in range(num_keypoints):
            heatmap = heatmaps[0, kp, :, :]
            y, x = np.unravel_index(np.argmax(heatmap), heatmap.shape)
            x = int(x * scale_x)
            y = int(y * scale_y)
    
            # í‚¤í¬ì¸íŠ¸ë¥¼ ì›ë³¸ ì´ë¯¸ì§€ ìœ„ì— ê·¸ë¦¬ê¸°
            cv2.circle(image, (x, y), 5, (0, 255, 0), -1)
    
        return image
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì— ëŒ€í•´ ì‹¤í–‰
    image_path = "path/to/your/image.jpg"  # ì‹¤ì œ ì´ë¯¸ì§€ ê²½ë¡œ
    heatmaps = predict_pose(image_path)
    result_image = draw_keypoints(image_path, heatmaps)
    
    # ê²°ê³¼ ì €ì¥ ë° ì¶œë ¥
    cv2.imwrite("result.jpg", cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR))
    cv2.imshow("Pose Estimation", result_image)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

* * *

    âœ… Inference completed! Heatmap shape: (1, 17, 64, 64)
    í‚¤í¬ì¸íŠ¸ 0: (0, 0)
    í‚¤í¬ì¸íŠ¸ 1: (0, 0)
    í‚¤í¬ì¸íŠ¸ 2: (0, 0)
    í‚¤í¬ì¸íŠ¸ 3: (0, 0)
    í‚¤í¬ì¸íŠ¸ 4: (0, 0)
    í‚¤í¬ì¸íŠ¸ 5: (0, 0)
    í‚¤í¬ì¸íŠ¸ 6: (0, 0)
    í‚¤í¬ì¸íŠ¸ 7: (0, 0)
    í‚¤í¬ì¸íŠ¸ 8: (0, 0)
    í‚¤í¬ì¸íŠ¸ 9: (0, 0)
    í‚¤í¬ì¸íŠ¸ 10: (0, 0)
    í‚¤í¬ì¸íŠ¸ 11: (0, 0)
    í‚¤í¬ì¸íŠ¸ 12: (0, 0)
    í‚¤í¬ì¸íŠ¸ 13: (0, 0)
    í‚¤í¬ì¸íŠ¸ 14: (0, 0)
    í‚¤í¬ì¸íŠ¸ 15: (0, 0)
    í‚¤í¬ì¸íŠ¸ 16: (0, 0)

ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤!Â  í‚¤í¬ì¸íŠ¸ë¥¼ ì œëŒ€ë¡œ ì°¾ì§€ ëª»í•˜ëŠ” ê²ƒ ê°™ì•„ì„œ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.

ì›ì¸ì„ ì˜ˆìƒí•´ë³´ê³  í•˜ë‚˜ì”© ì ‘ê·¼í•´ ë³´ê² ìŠµë‹ˆë‹¤.

#### **1\. ëª¨ë¸ ì˜ˆì¸¡ê°’ì´ ì „ë¶€ 0**  
np.max(heatmaps) ì¶œë ¥í•˜ì—¬ Heatmap ê°’ í™•ì¸

![](https://blog.kakaocdn.net/dna/dXJEt6/btsL9n9zzBU/AAAAAAAAAAAAAAAAAAAAADUHoHfBARctrbQOuQsLluS9KZb8DIHE-MUb4DKKE4ra/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=4cwNCl0wM7mTlZ4sHnKS%2B8n0Uaw%3D)

ğŸ”¹ **í˜„ì¬ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ Heatmap ê°’ì´ ë„ˆë¬´ ì‘ì•„ì„œ, í‚¤í¬ì¸íŠ¸ë¥¼ ì°¾ê¸° ì–´ë ¤ì›€**  
ğŸ”¹ **Heatmapì„ ë³´ì •í•˜ì—¬, í‚¤í¬ì¸íŠ¸ë¥¼ ë” ëšœë ·í•˜ê²Œ ì°¾ë„ë¡ ê°œì„ í•´ì•¼ í•¨**í˜„ì¬ np.argmax(heatmap)ì„ ì‚¬ìš©í•  ë•Œ, **ê°’ì´ ì‘ì•„ì„œ ê°•í•œ ì‹ í˜¸ë¥¼ ì°¾ê¸° ì–´ë ¤ì›€**  
ğŸ‘‰ **Heatmapì„ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”í•˜ë©´, ìƒëŒ€ì ì¸ ê°•í•œ í‚¤í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŒ!**  
ğŸ”¹ Â ******Heatmapì„ ì •ê·œí™”í•˜ì—¬ ê°•í•œ ì‹ í˜¸ ê°ì§€******

        for kp in range(num_keypoints):
            heatmap = heatmaps[0, kp, :, :]
    
            # ğŸ”¹ Heatmapì„ ì •ê·œí™”í•˜ì—¬ ìµœëŒ€ê°’ì´ 1ì´ ë˜ë„ë¡ ì¡°ì •
            heatmap = (heatmap - np.min(heatmap)) / (np.max(heatmap) - np.min(heatmap) + 1e-9)
    
            # ğŸ”¹ ì •ê·œí™”ëœ Heatmapì—ì„œ ê°€ì¥ ê°•í•œ ì‹ í˜¸ ìœ„ì¹˜ ì°¾ê¸°
            y, x = np.unravel_index(np.argmax(heatmap), heatmap.shape)
            x = int(x * scale_x)
            y = int(y * scale_y)

![](https://blog.kakaocdn.net/dna/ctFYoM/btsMaT0xg8c/AAAAAAAAAAAAAAAAAAAAAPaVDvQA2Kl6ZHSUZfqcB1rptg3TJ1s-pqSuOLjuCKHB/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=jYhAr760zoUiAKdwd1c7fHI1Na4%3D)

í•´ê²°í•˜ì§€ ëª»í–ˆë‹¤...  
  
  
í•˜ì§€ë§Œ ì˜ëª»ëœ ìƒ˜í”Œì„ ê°€ì ¸ì™”ì„ ìˆ˜ ìˆë‹¤ëŠ” ìƒê°ì— í‚¤í¬ì¸íŠ¸ê°€ ë§ì€ ì‚¬ì§„ì„ ì°¾ì•„ë³´ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤.

    from pycocotools.coco import COCO
    
    # ì–´ë…¸í…Œì´ì…˜ íŒŒì¼ ë¡œë“œ (í›ˆë ¨ or ê²€ì¦ ë°ì´í„°ì…‹ ì„ íƒ)
    ann_file = 
    coco = COCO(ann_file)
    
    # ëª¨ë“  ì´ë¯¸ì§€ ID ê°€ì ¸ì˜¤ê¸°
    img_ids = coco.getImgIds()
    
    # í‚¤í¬ì¸íŠ¸ ê°œìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    img_keypoints = []
    for img_id in img_ids:
        ann_ids = coco.getAnnIds(imgIds=img_id, iscrowd=False)
        anns = coco.loadAnns(ann_ids)
        
        # í˜„ì¬ ì´ë¯¸ì§€ì˜ ëª¨ë“  ì‚¬ëŒì— ëŒ€í•´ í‚¤í¬ì¸íŠ¸ ê°œìˆ˜ í•©ì‚°
        total_keypoints = sum([ann["num_keypoints"] for ann in anns if "num_keypoints" in ann])
        img_keypoints.append((img_id, total_keypoints))
    
    # í‚¤í¬ì¸íŠ¸ê°€ ë§ì€ ìƒìœ„ 5ê°œ ì´ë¯¸ì§€ ì¶œë ¥
    img_keypoints = sorted(img_keypoints, key=lambda x: x[1], reverse=True)[:5]
    print("í‚¤í¬ì¸íŠ¸ ë§ì€ ìƒìœ„ 5ê°œ ì´ë¯¸ì§€:", img_keypoints)

![](https://blog.kakaocdn.net/dna/ttrCd/btsMalJ0A1p/AAAAAAAAAAAAAAAAAAAAAIx8LO9Ski1lTpMfAkkIrY8hWWqBMdGu_VaO0yniFsji/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=kTLdT%2FTwtF3e3cDgLFOWjBDhCKQ%3D)

![](https://blog.kakaocdn.net/dna/Q48Uq/btsL9pl3xs2/AAAAAAAAAAAAAAAAAAAAAAQ7jfV00yMjcf_CPpH2DQWalW56unADFO0I6v0SlG7h/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=VcYjeelUHDANVWhWdPEYB0pfqNo%3D)

í•˜ì§€ë§Œ ì—¬ì „íˆ ì˜ ì°¾ì•„ë‚´ì§€ ëª»í•˜ê³  ìˆìŠµë‹ˆë‹¤.  
  
í•™ìŠµìƒíƒœë¥¼ ì˜ì‹¬í•´ë³´ì•˜ìŠµë‹ˆë‹¤.

    import torch
    from simple_pose_net import SimplePoseNet
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # ëª¨ë¸ ë¡œë“œ
    model = SimplePoseNet(num_keypoints=17).to(device)
    try:
        model.load_state_dict(torch.load("pose_model.pth", map_location=device))
        print("âœ… ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨:", e)
    
    # ê°€ì¤‘ì¹˜ í™•ì¸ (ì¼ë¶€ ë ˆì´ì–´ ê°’ ì¶œë ¥)
    for name, param in model.named_parameters():
        print(f"{name}: {param.mean().item():.6f}")
        break  # ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šë„ë¡ ì²« ë²ˆì§¸ ë ˆì´ì–´ë§Œ ì¶œë ¥

![](https://blog.kakaocdn.net/dna/FGFiA/btsL9QwZwk6/AAAAAAAAAAAAAAAAAAAAAJtsa5esqjG3TMMhZ63E-Sq4sjjJD89Dgp9HPlmgdRet/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=8WwZUcpQK3G2FhBLOZO34mtfVtU%3D)

ì•„ë¬´ë˜ë„ í•™ìŠµë°˜ë³µì„ 2íšŒë§Œ í•´ì„œ ê·¸ëŸ° ê²ƒ ê°™ìŠµë‹ˆë‹¤.

![](https://blog.kakaocdn.net/dna/bAsHMv/btsMcrb1F02/AAAAAAAAAAAAAAAAAAAAAIwNmcGt4hxiMmD6JOdvQqCqwWOVkgqZi8ZqIs2LyewD/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=bzQmZKwcBqmMOcCDep2YX6j0Etg%3D)

ë°”ë¡œ íšŸìˆ˜ì˜ ë¬¸ì œê°€ ì•„ë‹˜ì„ ì•Œ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.Â   
  

**í˜„ì¬ ë¬¸ì œ ë¶„ì„**
------------

### ğŸ”¹ **Lossê°€ 0.0010ì—ì„œ ì¤„ì–´ë“¤ì§€ ì•ŠëŠ” ì´ìœ ?**

1ï¸âƒ£ **ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœí•´ì„œ ë³µì¡í•œ Pose Estimationì„ í•™ìŠµí•˜ì§€ ëª»í•  ê°€ëŠ¥ì„±**  
â†’ í˜„ì¬ SimplePoseNetì´ ë„ˆë¬´ ê°„ë‹¨í•´ì„œ, ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬ê°€ í•„ìš”í•  ìˆ˜ë„ ìˆìŒ

2ï¸âƒ£ **í•™ìŠµë¥ (Learning Rate)ì´ ë„ˆë¬´ ì»¤ì„œ í•™ìŠµì´ ìˆ˜ë ´í•˜ì§€ ì•ŠìŒ**  
â†’ lr=1e-3ì€ Pose Estimationì—ì„œëŠ” ë„ˆë¬´ í´ ìˆ˜ë„ ìˆìŒ  
â†’ **1e-4ë¡œ ì¤„ì´ë©´ í•™ìŠµì´ ë” ì•ˆì •ì ìœ¼ë¡œ ì§„í–‰ë  ê°€ëŠ¥ì„±**

3ï¸âƒ£ **ì†ì‹¤ í•¨ìˆ˜(MSELoss)ê°€ ì ì ˆí•˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±**  
â†’ MSELoss ëŒ€ì‹  Smooth L1 Loss(HuberLoss) ë˜ëŠ” KLDivLoss ì‹œë„ ê°€ëŠ¥

4ï¸âƒ£ **ë°ì´í„° ì „ì²˜ë¦¬(Preprocessing) ë¬¸ì œ**  
â†’ Heatmapì´ ë„ˆë¬´ ì‘ì€ í¬ê¸°(64x64)ë¡œ ì¶•ì†Œë˜ë©´ì„œ ì •ë³´ê°€ ì†ì‹¤ë˜ì—ˆì„ ê°€ëŠ¥ì„±

ì¡°ê¸ˆ ë” ë³µì¡í•œ ëª¨ë¸ì„ êµ¬ì„±í•´ ë³´ê² ìŠµë‹ˆë‹¤.

    import torchvision.models as models
    
    class ResNetPoseNet(nn.Module):
        def __init__(self, num_keypoints=17):
            super(ResNetPoseNet, self).__init__()
            self.backbone = models.resnet18(pretrained=True)  # ResNet18 ì‚¬ìš©
            self.backbone.fc = nn.Linear(512, num_keypoints * 2)  # 17ê°œ ê´€ì ˆ * (x, y)
        
        def forward(self, x):
            x = self.backbone(x)
            return x.view(-1, 17, 2)  # (batch, num_keypoints, 2)

**ë¹„êµ ë¶„ì„**
---------

ëª¨ë¸ë ˆì´ì–´ ê°œìˆ˜íŒŒë¼ë¯¸í„° ìˆ˜(approx)íŠ¹ì§•

**SimplePoseNet**

4ê°œ (Conv + BN + Deconv)

**ì•½ 1~2M**

ë‹¨ìˆœí•œ CNN êµ¬ì¡°, ì‘ì€ ë°ì´í„°ì…‹ì—ì„œ í•™ìŠµ ê°€ëŠ¥

**ResNetPoseNet (ResNet18 ê¸°ë°˜)**

18ê°œ ì´ìƒ (ResNet18)

**ì•½ 11M**

ê°•ë ¥í•œ Feature Extractor, ë³µì¡í•œ ë°ì´í„° í•™ìŠµ ê°€ëŠ¥

### **ResNet ê¸°ë°˜ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ë³µì¡í•´ì¡ŒëŠ”ê°€?**

1\. **SimplePoseNet** ì€ ì‘ì€ CNN ë„¤íŠ¸ì›Œí¬ë¡œ, **4ê°œì˜ ì£¼ìš” ë ˆì´ì–´**(Conv + BN + Deconv)ë§Œ ì¡´ì¬  
2\. **ResNetPoseNet** ì€ **18ê°œ ì´ìƒì˜ ë ˆì´ì–´(ResNet18 êµ¬ì¡°)** ë¥¼ í¬í•¨, **11M ì´ìƒì˜ íŒŒë¼ë¯¸í„°**ë¥¼ ê°€ì§  
3\. ResNetì€ **Skip Connection**ì„ í¬í•¨ â†’ **Gradient ì†Œì‹¤ ë°©ì§€ & ë” ê¹Šì€ íŠ¹ì§• í•™ìŠµ ê°€ëŠ¥**  
4.ê¸°ì¡´ ëª¨ë¸ì€ **Heatmapì„ ì˜ˆì¸¡**í–ˆì§€ë§Œ, **ResNetPoseNetì€ ì§ì ‘ (x, y) ì¢Œí‘œë¥¼ ì˜ˆì¸¡  
  
**

**ResNetPoseNet ì ìš© í›„ ê¸°ëŒ€ íš¨ê³¼**
----------------------------

ëª¨ë¸í•™ìŠµ ì†ë„ì •í™•ë„(ì˜ˆìƒ)íŠ¹ì§•

**SimplePoseNet**

ë¹ ë¦„ (ê°„ë‹¨í•œ ë„¤íŠ¸ì›Œí¬)

ë‚®ìŒ

ì—°ì‚°ëŸ‰ì´ ì‘ì•„ ë¹ ë¥´ì§€ë§Œ ë³µì¡í•œ í¬ì¦ˆë¥¼ ì˜ ëª» ì°¾ì„ ê°€ëŠ¥ì„±

**ResNetPoseNet**

ëŠë¦¼ (ë³µì¡í•œ ë„¤íŠ¸ì›Œí¬)

ë†’ìŒ

ResNet ê¸°ë°˜ ê°•ë ¥í•œ íŠ¹ì§• ì¶”ì¶œë¡œ í¬ì¦ˆë¥¼ ì •í™•í•˜ê²Œ ì˜ˆì¸¡ ê°€ëŠ¥

1\. **ResNetPoseNetì„ ì‚¬ìš©í•˜ë©´ ë” ì •êµí•œ í¬ì¦ˆ ì¸ì‹ ê°€ëŠ¥!**  
2\. **ë‹¤ë§Œ í•™ìŠµ ì†ë„ëŠ” ë‹¤ì†Œ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŒ** (í•˜ì§€ë§Œ ì •í™•ë„ í–¥ìƒì„ ìœ„í•´ ì¶©ë¶„í•œ Trade-off)

![](https://blog.kakaocdn.net/dna/cl0dA6/btsMaBAzhL0/AAAAAAAAAAAAAAAAAAAAAAvbk9tze-8K-9y3xrSAz1CYGMZBl1P3UcO6Oh8VRMtx/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=Csx0nRI5n6zHWfzkhyXaACQgtX8%3D)

ë²”ìœ„ë¥¼ 0~1ë¡œ ì„¤ì •í•˜ì§€ ì•Šì•„ì„œ ìˆ˜ì¹˜ê°€ ì»¤ì¡Œì§€ë§Œ ê³„ì† ë¡œìŠ¤ê°’ì´ ì¤„ì–´ë“¤ê³  ìˆìŒì„ í™•ì¸

![](https://blog.kakaocdn.net/dna/cosnKW/btsMcdrzA4W/AAAAAAAAAAAAAAAAAAAAAArzucJaD_prLLYWSZyOTuFz8p-i0ckcPNXWEPx1kEvR/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=W3cCroC5HNJ0tlFXkl5QoKKQWCk%3D)

ì¶”ë¡ ì„ ì™„ë£Œí–ˆê³  ì´ì œ ê²°ê³¼ë¥¼ ì‹œê°í™” í•´ë´…ì‹œë‹¤.

    import cv2
    import numpy as np
    import torch
    from PIL import Image
    import matplotlib.pyplot as plt
    
    # ì¶”ë¡  ê²°ê³¼ ë¡œë“œ
    predicted_keypoints = np.array([
        [396.91986, 125.40627], [384.28397, 113.19349], [357.68848, 108.36974],
        [378.0459, 100.082695], [147.31769, 72.80783], [439.58835, 154.01808],
        [393.80966, 156.50447], [448.84732, 205.02428], [374.3959, 203.47243],
        [439.41943, 224.10767], [366.96207, 216.2085], [441.38068, 272.10345],
        [408.06808, 271.70392], [398.31348, 299.77182], [371.58908, 297.96512],
        [379.52823, 351.85037], [355.95764, 350.56714]
    ])
    
    # ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ 
    image_path = "ê²½ë¡œ"
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # OpenCVëŠ” BGRì´ë¯€ë¡œ RGBë¡œ ë³€í™˜
    
    # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
    for (x, y) in predicted_keypoints:
        cv2.circle(image, (int(x), int(y)), 5, (255, 0, 0), -1)  # ğŸ”µ íŒŒë€ìƒ‰ ì 
    
    # ê²°ê³¼ ì´ë¯¸ì§€ ì¶œë ¥
    plt.figure(figsize=(8, 6))
    plt.imshow(image)
    plt.axis("off")
    plt.title("Pose Estimation Keypoints")
    plt.show()

![](https://blog.kakaocdn.net/dna/clWCaN/btsMa9Q7TqJ/AAAAAAAAAAAAAAAAAAAAAMAmaWTJt8KzY2SkO34x0_cBCZgvXIJcSr7MOoeoEy-H/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=Uc6oODcd0CvQPdVmJwxaVRAvDzg%3D)

ì •í™•ë„ ì´ìŠˆëŠ” ìˆì§€ë§Œ ì–´ëŠì •ë„ ì ì¤‘ë¥ ì„ ë³´ì¸ë‹¤ëŠ” ê²ƒì€ ë°©í–¥ì„±ì„ ì˜ ì¡ê³  ìˆë‹¤ëŠ” ëœ»ìœ¼ë¡œ í•´ì„ì´ ëœë‹¤!!

* * *

**ğŸ“Œ ì¶”ê°€ ê³ ë„í™” ê³„íš**
----------------

âœ… **1\. 224x224 ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ìµœì í™”**

*   ResNet ëª¨ë¸ì€ ê¸°ë³¸ì ìœ¼ë¡œ **(224, 224)** í¬ê¸°ì˜ ì…ë ¥ì„ ë°›ë„ë¡ ì„¤ê³„ë¨
*   ë”°ë¼ì„œ ì „ì²˜ë¦¬ì—ì„œ ì´ë¯¸ì§€ í¬ê¸°ë¥¼ **(256, 256) â†’ (224, 224)** ë¡œ ë³€ê²½
*   **ìˆ˜ì • ì½”ë“œ (coco\_pose\_dataset.py)**

    # coco_pose_dataset.py
    
    self.input_size = (224, 224)  # ê¸°ì¡´ (256, 256) â†’ (224, 224)
    transforms.Resize(self.input_size)

âœ… **2\. í‚¤í¬ì¸íŠ¸ ì •ê·œí™”**

*   ì›ë³¸ **í”½ì…€ ì¢Œí‘œë¥¼ (0~1) ë²”ìœ„ë¡œ ì •ê·œí™”**í•˜ë©´ í•™ìŠµì´ ë” ì•ˆì •ì 
*   **ìˆ˜ì • ì½”ë“œ (coco\_pose\_dataset.py)**

    # coco_pose_dataset.py
    
    keypoints[:, 0] /= img_info["width"]   # x ì¢Œí‘œ ì •ê·œí™”
    keypoints[:, 1] /= img_info["height"]  # y ì¢Œí‘œ ì •ê·œí™”

âœ… **3\. ì†ì‹¤ í•¨ìˆ˜ ë³€ê²½ (MSELoss â†’ Smooth L1 Loss ì‚¬ìš©)**

*   MSELossëŠ” ì‘ì€ ì°¨ì´ì— ë¯¼ê°í•´ì„œ ë…¸ì´ì¦ˆê°€ í´ ê²½ìš° SmoothL1Lossê°€ ë” íš¨ê³¼ì 
*   **ìˆ˜ì • ì½”ë“œ (train.py)**

    # train.py
    
    criterion = nn.SmoothL1Loss()  # ê¸°ì¡´ MSELoss() â†’ SmoothL1Loss()

âœ… **4\. í•™ìŠµ íšŸìˆ˜ ì¦ê°€ (Epoch 7 â†’ 15)**

*   Lossê°€ ê³„ì† ì¤„ì–´ë“œëŠ” ì¤‘ì´ë¯€ë¡œ ì¶”ê°€ í•™ìŠµ ì§„í–‰
*   **ìˆ˜ì • ì½”ë“œ (train.py)**

python

    # train.py
    
    num_epochs = 15  # ê¸°ì¡´ 7 â†’ 15

âœ… **5\. í‚¤í¬ì¸íŠ¸ ì—°ê²°ì„  ì¶”ê°€ (ì‹œê°í™” ê°œì„ )**

*   í‚¤í¬ì¸íŠ¸ ì ë§Œ í‘œì‹œí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ **ê´€ì ˆì„ ì„ ìœ¼ë¡œ ì—°ê²°**
*   **ìˆ˜ì • ì½”ë“œ (visualize.py)**

python

    # visualize.py
    
    # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
    for (x, y) in predicted_keypoints:
        cv2.circle(image, (int(x), int(y)), 5, (255, 0, 0), -1)  # ğŸ”µ íŒŒë€ìƒ‰ ì 
    
    # ê´€ì ˆ ì—°ê²°ì„  ì¶”ê°€
    for (i, j) in COCO_SKELETON:
        pt1 = tuple(predicted_keypoints[i].astype(int))
        pt2 = tuple(predicted_keypoints[j].astype(int))
        cv2.line(image, pt1, pt2, (0, 255, 0), 2)  # ğŸŸ¢ ì´ˆë¡ìƒ‰ ì„ ìœ¼ë¡œ ì—°ê²°

![](https://blog.kakaocdn.net/dna/nTFus/btsMbZUz2dm/AAAAAAAAAAAAAAAAAAAAAJ5o9on-BLUr4oh8kFjbxVi3a_14Ufw-61WVP1rwasIB/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=Gwi5KKFx%2BmQX0pv%2FddluOa9ew6Y%3D)

    # COCO Keypoint ì—°ê²° ì •ë³´
    COCO_SKELETON = [
        (0, 1), (0, 2), (1, 3), (2, 4),  # ì–¼êµ´ (ì½”-ëˆˆ, ëˆˆ-ê·€ ì—°ê²°)
        (5, 6), (5, 7), (6, 8), (7, 9), (8, 10),  # ìƒì²´ (ì–´ê¹¨-íŒ”ê¿ˆì¹˜-ì†ëª© ì—°ê²°)
        (5, 11), (6, 12), (11, 12),  # ëª¸í†µ (ì–´ê¹¨-ê³¨ë°˜ ì—°ê²°)
        (11, 13), (12, 14), (13, 15), (14, 16)  # ë‹¤ë¦¬ (ê³¨ë°˜-ë¬´ë¦-ë°œëª© ì—°ê²°)
    ]

* * *

**ê²°ê³¼**
------

![](https://blog.kakaocdn.net/dna/x1Ve8/btsMbsC4hBq/AAAAAAAAAAAAAAAAAAAAAOMEgEqkPGT6VD2k3AGErTxs1QNGhISCNXxN1gaaHlpx/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=vwQOheG2A41Iecp0gpJe%2BRuWMJU%3D)

![](https://blog.kakaocdn.net/dna/mJfHI/btsMbypC6A4/AAAAAAAAAAAAAAAAAAAAAM3UKGY_2U57v8q5kcXNdjP8I9CKc3dxp1lAL338XDCd/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=vpDLbMUB4iUVkjSLVAjvFJWVBr4%3D)

![](https://blog.kakaocdn.net/dna/P3Xhb/btsMbIr0WPI/AAAAAAAAAAAAAAAAAAAAAArrRUJC-8jyyKjyA9lNt1DrYF_fkXz7SgLnIyCaFOnd/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=sukTQZyHh63zcJfoqhVOJ3KsS%2BM%3D)

![](https://blog.kakaocdn.net/dna/RM1Hi/btsMccGenNi/AAAAAAAAAAAAAAAAAAAAAN-KFzgcxqpOlDMAAzkn1FEMbUeRoidbsRAOQcPlazxH/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=%2FD89dY2qoWKXEOaiOpVuGgJUXn0%3D)

![](https://blog.kakaocdn.net/dna/bha3qf/btsMaPMfGhT/AAAAAAAAAAAAAAAAAAAAANIuUQzq96cEjlz2qbzbLC6PlLDDtkj3QIJmAyJml-wK/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=%2BZm8kK%2FLn%2Bz9DhDAcfdT0oMxIs0%3D)

![](https://blog.kakaocdn.net/dna/Z1v9h/btsMcsaY4Ns/AAAAAAAAAAAAAAAAAAAAAGmSe4cfQwwaLfdnHCdtFZR0stR5e9ZKvHdeEkyWtuX6/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=VDUfwAcBZ47jZk9OXUY5vmyVLCk%3D)

ì–¼êµ´ìª½ì€ ì •í™•íˆ ì¡ì•„ë‚´ì§€ ëª»í•˜ê³ (ê·¸ë˜ë„ ì¼ì •í•œ ê·œì¹™? ìœ¼ë¡œ íŠ€ëŠ” ê²ƒì„ í™•ì¸)  
ê·¸ ì™¸ì—ëŠ” ì–´ëŠì •ë„ í˜•íƒœë¥¼ ì˜ ì¡ì•„ë‚´ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.

ë¡œìŠ¤ìœ¨ì´ ê¾¸ì¤€íˆ ì¤„ì–´ë“œëŠ”ë° 15íšŒ ì„¤ì •í•œ ê²°ê³¼ë¬¼ì¸ê±¸ ê°ì•ˆí•˜ë©´

ì•½ 30íšŒ ì •ë„ ì¶”ê°€ í•™ìŠµì„ ì‹œí‚¨ë‹¤ë©´ ì •í™•ë„ê°€ ìƒë‹¹íˆ ì˜¬ë¼ê°ˆ ê²ƒìœ¼ë¡œ ê³ ë ¤ë©ë‹ˆë‹¤!

ê·¸ë¦¬ê³  ì–¼êµ´ìª½ì€ ì•„ë¬´ë˜ë„ cocoë°ì´í„°ì…‹ì˜ í•œê³„ì¸ ê²ƒìœ¼ë¡œ ì¶”ì •ë˜ê¸°ì— ì‹¤ìŠµì„ ë§ˆì¹˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤!

(adsbygoogle = window.adsbygoogle || \[\]).push({}); if(window.observeAdsenseUnfilledState !== undefined){ observeAdsenseUnfilledState(); }

window.ReactionButtonType = 'reaction'; window.ReactionApiUrl = '//eunmastudio.tistory.com/reaction'; window.ReactionReqBody = { entryId: 47 }

ê³µìœ í•˜ê¸°

ê²Œì‹œê¸€ ê´€ë¦¬

**EunmaStudio**

#### '[\[STUDY\]](/category/%5BSTUDY%5D) > [\[Single\_Project\]](/category/%5BSTUDY%5D/%5BSingle_Project%5D)' ì¹´í…Œê³ ë¦¬ì˜ ë‹¤ë¥¸ ê¸€

[\[CNN\] í•„ê¸°ì²´ ìˆ«ì ë¶„ë¥˜ ë”¥ëŸ¬ë‹ ëª¨ë¸](/17)Â Â (1)

2024.10.14
            