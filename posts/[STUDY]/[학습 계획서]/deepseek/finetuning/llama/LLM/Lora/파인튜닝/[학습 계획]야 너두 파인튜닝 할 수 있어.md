
# [학습 계획]야 너두 파인튜닝 할 수 있어

(adsbygoogle = window.adsbygoogle || \[\]).push({}); if(window.ObserveAdsenseUnfilledState !== undefined){ ObserveAdsenseUnfilledState(); }

### **1\. 프로젝트 개요**

*   **목표:** LLaMA 3.2 1B와 DeepSeek R1 1.5B 모델을 각각 한국어 학습시켜 성능을 비교
*   **주요 실험 내용:**
    *   동일한 한국어 데이터셋을 활용하여 두 모델을 LoRA 방식으로 파인튜닝
    *   한국어 문장이 어색하지 않도록 개선하는 것이 핵심 목표
    *   결과 비교를 통해 모델별 학습 효율성과 성능 차이 분석

* * *

### **2\. 모델 및 방법론 선정 이유**

#### **_모델 선정 이유_**

*   **LLaMA 3.2 1B & DeepSeek R1 1.5B**
    *   **이유:** 대형 LLM을 다루기 전에 **소규모 모델로 먼저 경험을 쌓기 위한 목적**
    *   Colab 환경에서 비교적 적은 자원으로 실험 가능
    *   두 모델 간 한국어 학습 성능 차이를 비교하는 데 적합

#### **_LoRA 선정 이유_**

*   **LoRA (Low-Rank Adaptation) 적용 이유**
    *   VRAM이 제한된 환경에서 **효율적으로 모델을 튜닝**할 수 있음
    *   기존의 Full Fine-tuning보다 **적은 데이터로도 빠르게 학습 가능**
    *   LLM의 주요 가중치를 그대로 유지하면서도, 특정 태스크에 맞게 적은 파라미터만 업데이트 가능

#### **_양자화(QLoRA) 적용 여부_**

*   QLoRA(4-bit Quantization)는 DeepSeek R1에서 지원하지만 LLaMA 3.2 1B에서는 제한적일 수 있음
*   두 모델 간 **공정한 비교를 위해 초기 실험에서는 양자화를 적용하지 않고 진행**
*   이후 실험 확장 시 **양자화를 적용한 버전과 비교할 가능성 고려**

* * *

### **3\. 데이터셋 선택 및 활용 전략**

#### **_1차 실험: 최소한의 데이터셋 활용_**

*   **데이터셋:** KLUE-NLI (Korean Natural Language Inference)
    *   다운로드 링크: [https://huggingface.co/datasets/klue](https://huggingface.co/datasets/klue)
    *   문장 간 논리적 관계 학습을 통해 자연스러운 한국어 응답을 생성하도록 훈련
*   **추후 확장 가능성:**
    *   초기 실험 결과를 바탕으로 추가 데이터셋(KorQuAD 1.0, AI Hub 대화체 등) 적용 고려

#### _**추가 고려 가능한 한국어 데이터셋**_

**데이터셋**

크기

특징

다운로드 링크

**KorQuAD 1.0**

4.5GB

위키 기반 QA 데이터셋

[https://korquad.github.io](https://korquad.github.io)

**AI Hub 대화체 데이터**

2GB~5GB

다양한 대화체 포함

[https://aihub.or.kr](https://aihub.or.kr)

**NSMC (Naver Sentiment Movie Corpus)**

150MB

감정 분석 (긍정/부정)

[https://github.com/e9t/nsmc](https://github.com/e9t/nsmc)

**Naver News Summarization**

1GB

뉴스 기사 요약 데이터

[https://huggingface.co/datasets/daekeun-ml/naver-news-summarization-ko](https://huggingface.co/datasets/daekeun-ml/naver-news-summarization-ko)

* * *

### **4\. 실험 방법 및 학습 환경**

*   **사용 모델:** LLaMA 3.2 1B vs DeepSeek R1 1.5B
*   **파인튜닝 기법:** LoRA (Low-Rank Adaptation)
*   **학습 환경:** Google Colab Pro (A100, 24GB VRAM 고려)
*   **Batch Size:** 초기 실험에서는 2~4로 설정 후 성능 평가

* * *

### **5\. 모델 성능 평가 방법**

1.  **자연스러움:** 한국어 문장이 번역체 느낌 없이 자연스러운가?
2.  **정확성:** 질문과 답변의 논리적 관계가 적절한가?
3.  **일관성:** 유사한 질문에 대해 일관된 답변을 제공하는가?
4.  **추론 능력:** KLUE-NLI의 자연어 추론 태스크에서 논리적 오류 없이 답을 생성하는가?

* * *

### **6\. 진행 일정**

단계

목표

예상 기간

1단계

KLUE-NLI 데이터셋 준비 및 전처리

1일

2단계

LLaMA 3.2 1B + LoRA 학습 진행

2~3일

3단계

DeepSeek R1 1.5B + LoRA 학습 진행

2~3일

4단계

학습된 모델 비교 및 평가

2일

5단계

추가 데이터셋 적용 여부 결정

이후 논의

* * *

### **7\. 기대 결과 및 활용 방안**

*   **모델별 한국어 학습 성능 차이 분석**
*   **적은 데이터로 한국어 튜닝이 가능한지 검증**
*   **이후 한국어 LLM 파인튜닝을 확장할 수 있는 전략 수립**

window.ReactionButtonType = 'reaction'; window.ReactionApiUrl = '//eunmastudio.tistory.com/reaction'; window.ReactionReqBody = { entryId: 55 }

공유하기

게시글 관리

**EunmaStudio**

#### '[\[STUDY\]](/category/%5BSTUDY%5D) > [\[학습 계획서\]](/category/%5BSTUDY%5D/%5B%ED%95%99%EC%8A%B5%20%EA%B3%84%ED%9A%8D%EC%84%9C%5D)' 카테고리의 다른 글

[\[학습 계획\] Github 기초부터 Master까지](/39)  (0)

2024.12.31

[\[학습 계획\] OCR](/38)  (2)

2024.12.30

[\[학습 계획\] 푸리에 변환](/37)  (1)

2024.12.19
            