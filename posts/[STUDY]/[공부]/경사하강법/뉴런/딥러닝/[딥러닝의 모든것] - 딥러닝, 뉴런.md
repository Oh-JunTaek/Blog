
# [딥러닝의 모든것] - 딥러닝, 뉴런

(adsbygoogle = window.adsbygoogle || \[\]).push({}); if(window.observeAdsenseUnfilledState !== undefined){ observeAdsenseUnfilledState(); }

**수강 강좌 : [https://www.udemy.com/course/best-artificial-neural-networks/?couponCode=JUST4U02223](https://www.udemy.com/course/best-artificial-neural-networks/?couponCode=JUST4U02223)**

![](https://blog.kakaocdn.net/dn/bQWvJz/btsMjVLP38h/mMyXfwBuRIiq4w1K5Ok8N1/img.png)

* * *

**학습 목표**
---------

이 강의 섹션에서는 **신경망(Neural Networks)의 기본 개념과 학습 방법**을 학습합니다. 주요 학습 목표는 다음과 같습니다.

1. **뉴런(The Neuron) - [53](https://eunmastudio.tistory.com/53)**

• 인공 신경망의 기본 단위인 뉴런(Neuron)의 구조와 동작 방식을 이해합니다.

2. **활성화 함수(The Activation Function) - [54](https://eunmastudio.tistory.com/54)**

• 뉴런의 출력을 결정하는 활성화 함수의 역할과 대표적인 함수(Sigmoid, ReLU, Tanh 등)를 살펴봅니다.

3. **신경망의 동작 방식(How do Neural Networks work?) - [55](https://eunmastudio.tistory.com/55)**

• 신경망이 입력 데이터를 처리하고 예측을 수행하는 과정을 실전 예제를 통해 알아봅니다.

4. **신경망의 학습 과정(How do Neural Networks learn?) - [55](https://eunmastudio.tistory.com/55)**

• 신경망이 데이터를 학습하는 과정과 오차를 줄이는 방법을 배웁니다.

5. **경사 하강법(Gradient Descent) - [56](https://eunmastudio.tistory.com/56 "56")**

• 비용 함수(Cost Function)를 최소화하기 위한 경사 하강법(Gradient Descent)의 개념과 적용 방법을 학습합니다.

6. **확률적 경사 하강법(Stochastic Gradient Descent)**\- [56](https://eunmastudio.tistory.com/56)****

• 전체 데이터가 아닌 일부 데이터를 사용하여 학습하는 Stochastic Gradient Descent(SGD)의 개념과 장점을 이해합니다.

7. **역전파(Backpropagation)**\- [57](https://eunmastudio.tistory.com/57)****

• 신경망이 가중치를 업데이트하는 핵심 알고리즘인 역전파(Backpropagation)의 원리를 익히고, 이를 통해 모델을 최적화하는 방법을 배웁니다.

* * *

_**딥러닝이란?**_ 
-------------

딥러닝은 **인공 신경망(Artificial Neural Networks, ANN)을 기반으로 한 머신러닝의 한 분야, **심층 신경망(Deep Neural Networks, DNN)을 활용하여 대량의 데이터에서 높은 차원의 표현 학습을 가능하게 하는 방법론****

전통적인 머신러닝은 특징(feature)을 사람이 직접 설계해야 하지만, 딥러닝은 데이터를 입력하면 모델이 스스로 특징을 학습

전통적인 ML보다 높은 표현력을 가지며, 컴퓨터 비전, NLP, 강화 학습(RL) 등의 다양한 분야에서 활용.

**핵심 개념:**

• **다층 퍼셉트론(MLP)**: 뉴런이 여러 층으로 연결된 기본 신경망

• **활성화 함수(Activation Function)**: ReLU, Sigmoid, Tanh 등의 비선형 함수

• **오차 역전파(Backpropagation)**: 가중치를 조정하여 학습하는 핵심 알고리즘

• **경사 하강법(Gradient Descent)**: 학습을 최적화하는 방법

**핵심 기술 스택:**

• **신경망 구조:** CNN(합성곱 신경망), RNN/LSTM/Transformer(순환 및 자기 주의 신경망), GAN(생성 모델)

• **최적화 기법:** Adam, RMSprop, Dropout, Batch Normalization

• **프레임워크:** TensorFlow, PyTorch, ONNX

• **실제 적용 사례:** 대규모 추천 시스템, 자율주행, AlphaGo, ChatGPT

**딥러닝의 강점**은 **비선형 변환을 통해 복잡한 데이터에서 높은 차원의 표현을 자동 학습**.

하지만 **고성능 GPU가 필요하고, 대량의 데이터를 요구하며, 과적합(overfitting) 문제를 신경 써야 한다는 단점**.

* * *

_**뉴런(Neuron)이란?**_
-------------------

뉴런은 **신경망(Neural Network)의 기본 단위**이며, 인간 뇌의 뉴런을 모방하여 만든 연산 노드.

#### **생물학적 뉴런 vs 인공 뉴런**

• **실제 뉴런:** 뉴런은 가지돌기(Dendrite)와 축삭돌기(Axon)로 연결되어 신호를 전달.by 산티아고 라몬 이 카할의 연구1899

• **인공 뉴런:** 인간 뉴런을 본떠서 만든 수학적 모델로, 입력 값을 받고 가공하여 다음 뉴런으로 신호를 전달함.

* * *

#### **뉴런의 주요 구성 요소**

**인공 신경망에서 뉴런의 역할**

**입력(Input) & 가중치(Weight)**

• 뉴런은 **여러 개의 입력**을 수신. (예: 이미지 픽셀, 숫자 데이터)

• 각각의 입력에는 **가중치(Weight)** 가 곱해져서 **중요한 값은 강조, 덜 중요한 값은 감소**함.

z = W X + b

• X  : 입력 값

• W  : 가중치(Weight)

• b  : 바이어스(Bias)

#### **시냅스(Synapse) - 뉴런 간 연결**

• 생물학적 뉴런에서는 **축삭돌기(Axon)와 가지돌기(Dendrite)** 가 물리적으로 연결되지 않고, 전기 신호를 통해 정보를 전달함.

• 인공 신경망에서는 **이 연결을 시냅스(Synapse)라고 부르고, 가중치와 함께 학습하는 요소**로 활용.

**활성화 함수(Activation Function) 적용**

• 단순한 선형 변환만으로는 복잡한 패턴을 학습할 수 없음.

• 활성화 함수를 적용해서 **비선형성(Non-linearity)을 추가**하면 신경망이 더 복잡한 관계를 학습할 수 있음.

y = f(WX + b)

**대표적인 활성화 함수**

• **Sigmoid**: 0과 1 사이의 값 → 확률 기반 예측에 적합

![](https://blog.kakaocdn.net/dn/cj5IRd/btsMksJqXkp/oWBD7atdVpuCTvK0damXk0/img.png)

  
  

• **ReLU**: 음수를 0으로 처리하고 양수는 그대로 전달 → 딥러닝에서 가장 많이 사용됨

• **Tanh**: -1 ~ 1 사이의 값 → 중간값을 고려하는 모델에 적합

![](https://blog.kakaocdn.net/dn/oE5ma/btsMlN6MSoe/KPI7RoNfxQc5wCZC4ee601/img.png)

**출력(Output) - 다음 층으로 신호 전달**

• 뉴런이 활성화 함수 결과를 출력하면, 이 값이 다음 층의 뉴런 입력으로 전달됨.

• 결국 신경망을 구성하는 뉴런들이 서로 연결되면서 정보가 흐름.

* * *

#### **뉴런의 학습 과정 (역전파와 가중치 조절)**

뉴런이 학습하기 위해서는 **오류를 줄이는 방향으로 가중치를 조절**.

이 과정에서 **역전파(Backpropagation)** 가 핵심적인 역할을 함!

**(1) 순전파(Forward Propagation)**

• 입력 데이터가 뉴런을 통해 전달되면서 연산 수행

• 활성화 함수가 적용되어 최종 출력값(\\hat{y}) 생성

• 예측 값(\\hat{y})과 실제 값(y)의 차이를 손실 함수(Loss Function)로 계산

**(2) 역전파(Backpropagation)**

• 손실 함수의 결과를 기반으로 **각 가중치가 얼마나 오차에 영향을 줬는지 계산**

• **경사 하강법(Gradient Descent)** 을 사용하여 가중치를 조금씩 수정

![](https://blog.kakaocdn.net/dn/Lb1LL/btsMkzhfLwJ/qDpDZRPKh1E52rJHZC65uK/img.png)

• \\alpha : 학습률(Learning Rate)

•

![](https://blog.kakaocdn.net/dn/evmcoM/btsMkcs5JMP/Pl9nGhFtZ1B28ESmvE8Ezk/img.png)

 : 손실 함수에 대한 가중치의 변화량

• 이 과정을 반복하면서 신경망이 점점 더 **정확한 예측을 할 수 있도록 최적의 가중치를 찾음**.

* * *

#### **뉴런을 조합하여 신경망(Network) 만들기**

**단일 뉴런 vs 다층 신경망(MLP, CNN, RNN 등)**

• **단일 뉴런** → 단순한 연산 가능 (선형 회귀와 유사)

• **다층 신경망(Multi-Layer Perceptron, MLP)** → 여러 층의 뉴런을 연결하여 복잡한 문제 해결 가능

• **CNN, RNN, 트랜스포머** 같은 고급 신경망 구조도 뉴런을 조합하여 만들어짐!

*   입력층(Input Layer): 데이터를 받아들이는 첫 번째 층
*   은닉층(Hidden Layer): 입력값을 가공하여 중요한 특징을 추출하는 층
*   출력층(Output Layer): 최종 결과를 출력하는 층

* * *

**핵심 요약**

뉴런은 신경망의 **가장 기본적인 연산 단위**

가중치(Weight)와 활성화 함수(Activation Function)를 사용하여 데이터를 변환

**순전파 → 손실 계산 → 역전파** 를 반복하며 학습 진행

여러 뉴런을 조합하여 복잡한 패턴을 학습하는 **다층 신경망(MLP, CNN, RNN 등)** 을 형성

* * *

**개념 정리**

#### **1\. 경사 하강법 (Gradient Descent)**

**정의**

*   경사 하강법은 신경망의 비용 함수(Cost Function) 를 최소화하여 최적의 가중치(Weight)를 찾기 위한 최적화 알고리즘이다.

**기능**

*   비용 함수의 기울기를 따라 가중치를 조정하여 점점 더 좋은 모델을 만들도록 학습을 진행한다.
*   학습률(Learning Rate, \\alpha)을 조정하여 한 번에 이동하는 크기를 결정한다.

**목적**

*   신경망이 예측값과 실제값의 차이를 줄이면서 점진적으로 최적화된 상태를 찾도록 함.

#### **2\. 확률적 경사 하강법 (Stochastic Gradient Descent, SGD)**

**정의**

*   경사 하강법의 변형으로, 전체 데이터셋이 아닌 일부 데이터(미니배치 또는 샘플)를 사용하여 가중치를 업데이트하는 방법.

**기능**

*   계산량이 줄어 학습 속도가 빨라지고, 더 빠르게 수렴할 수 있음.
*   하지만 변동성이 커서 최적의 값을 지나칠 위험도 있음.

**목적**

*   대량의 데이터를 학습할 때 계산 비용을 줄이면서 모델의 성능을 개선하기 위함.

#### **3\. 역전파 (Backpropagation)**

**정의**

*   신경망의 가중치를 업데이트하는 알고리즘으로, 오차가 어떻게 발생했는지를 거꾸로 추적하여 각 가중치가 오차에 얼마나 영향을 미쳤는지 계산하는 과정.

**기능**

*   **순전파(Forward Propagation):** 입력 데이터를 통해 예측값을 생성.
*   **오차 계산:** 예측값과 실제값의 차이를 비용 함수로 측정.
*   **역전파:** 오차를 미분하여 가중치를 수정할 방향을 계산.
*   **가중치 업데이트:** 경사 하강법을 사용하여 가중치를 조정.  
    **목적:**
*   네트워크가 점점 더 정확한 예측을 하도록 학습시키기 위함.

#### **4\. 활성화 함수 (Activation Function)**

**정의**

*   뉴런의 출력을 결정하는 비선형 함수로, 신경망이 복잡한 패턴을 학습할 수 있도록 도와줌.

**대표적인 활성화 함수**

*   **시그모이드(Sigmoid):** 0~1 사이 값 출력 → 확률 기반 예측에 적합.
*   **ReLU(Rectified Linear Unit):** 음수를 0으로, 양수를 그대로 유지 → 딥러닝에서 가장 많이 사용됨.
*   **Tanh (Hyperbolic Tangent):** -1~1 사이 값 출력 → 데이터의 중심이 0인 경우 적절.  
    **목적:**
*   신경망에 비선형성을 추가하여 복잡한 패턴을 학습할 수 있도록 함.

#### **5\. 다층 퍼셉트론 (MLP, Multi-Layer Perceptron)**

**정의**

*   뉴런이 여러 층으로 연결된 신경망의 기본 구조로, 입력층(Input Layer), 은닉층(Hidden Layer), 출력층(Output Layer)으로 구성됨.

**기능**

*   단일 뉴런(Perceptron)은 단순한 연산만 가능하지만, 다층 신경망을 사용하면 복잡한 문제 해결 가능.  
    **목적:**
*   이미지 인식, 자연어 처리 등 다양한 복잡한 문제 해결.

#### **6\. 비용 함수 (Cost Function)**

**정의**

*   모델의 예측값과 실제값의 차이를 측정하는 함수로, 학습 과정에서 최적화를 위한 기준이 됨.

**예시**

*   교차 엔트로피(Cross-Entropy Loss): 분류 문제에서 자주 사용됨.
*   평균 제곱 오차(Mean Squared Error, MSE)

![](https://blog.kakaocdn.net/dn/bagS5w/btsMkIkHVQS/yKmlo4pkkywECKjYmBHhj1/img.png)

**목적**

*   신경망이 예측값을 실제값과 최대한 가깝게 만들도록 학습하는 기준 제공.

**7\. 선형 변환 (Linear Transformation)**

**정의**

*   입력 데이터에 대해 선형 함수(Linear Function)를 적용하여 변환하는 과정.
*   수학적으로는 행렬 곱셈을 통해 표현됨.

y = W X + b

*   X: 입력 데이터 (벡터 또는 행렬)
*   W: 가중치 행렬 (Weight Matrix)
*   b: 바이어스 (Bias)
*   y: 변환된 출력 값

**기능**

*   입력 데이터를 일정한 규칙에 따라 변형하여 더 나은 표현을 만들도록 함.
*   벡터의 회전, 확대, 축소 등의 연산을 포함.

**예시**

*   이미지 데이터에서 픽셀 값을 정규화할 때 사용.
*   신경망의 각 층에서 입력을 가중치와 곱하여 다음 층으로 전달하는 과정.

**8\. 비선형성 (Non-Linearity)**

**정의**

*   데이터가 단순한 직선(선형 관계)으로 표현되지 않는 성질.
*   신경망에서 활성화 함수(Activation Function)를 추가하는 이유가 바로 비선형성을 적용하기 위함.

**기능**

*   신경망이 더 복잡한 패턴을 학습할 수 있도록 도움.
*   선형 모델은 단순한 관계만 표현 가능하지만, 비선형성을 추가하면 더 다양한 문제 해결 가능.

**예시**

*   이미지 분류: 단순히 픽셀 값의 합으로는 고양이와 개를 구분할 수 없음.
*   음성 인식: 주파수 신호만으로는 단어를 구분하기 어려움 → 신경망이 비선형 변환을 적용하여 구별.

* * *

#### **다음 내용**

[딥러닝의 모든것 - 활성화 함수](https://eunmastudio.tistory.com/54)

 [\[딥러닝의 모든것\] - 활성화 함수

수강 강좌 : https://www.udemy.com/course/best-artificial-neural-networks/?couponCode=JUST4U02223 이전 학습 내용\[딥러닝의 모든것\] - 딥러닝, 뉴런 \[딥러닝의 모든것\] - 딥러닝, 뉴런수강 강좌 : https://www.udemy.com/c

eunmastudio.tistory.com](https://eunmastudio.tistory.com/54)

window.ReactionButtonType = 'reaction'; window.ReactionApiUrl = '//eunmastudio.tistory.com/reaction'; window.ReactionReqBody = { entryId: 53 }

공유하기

게시글 관리

**EunmaStudio**

#### '[\[STUDY\]](/category/%5BSTUDY%5D) > [\[공부\]](/category/%5BSTUDY%5D/%5B%EA%B3%B5%EB%B6%80%5D)' 카테고리의 다른 글

[\[딥러닝의 모든것\] - 활성화 함수](/54)  (0)

2025.02.16
            