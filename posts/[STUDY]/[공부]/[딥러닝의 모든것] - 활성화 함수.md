
# [딥러닝의 모든것] - 활성화 함수

**수강 강좌 : [https://www.udemy.com/course/best-artificial-neural-networks/?couponCode=JUST4U02223](https://www.udemy.com/course/best-artificial-neural-networks/?couponCode=JUST4U02223)**

![](https://blog.kakaocdn.net/dn/JfiT9/btsMkKb3vum/MqZByDPaa5R8Hbksdp2Kmk/tfile.dat)

* * *

**학습 목표**
---------

이 강의 섹션에서는 **신경망(Neural Networks)의 기본 개념과 학습 방법**을 학습합니다. 주요 학습 목표는 다음과 같습니다.

1. **뉴런(The Neuron) - [53](https://eunmastudio.tistory.com/53)**

• 인공 신경망의 기본 단위인 뉴런(Neuron)의 구조와 동작 방식을 이해합니다.

2. **활성화 함수(The Activation Function) - [54](https://eunmastudio.tistory.com/54)**

• 뉴런의 출력을 결정하는 활성화 함수의 역할과 대표적인 함수(Sigmoid, ReLU, Tanh 등)를 살펴봅니다.

3. **신경망의 동작 방식(How do Neural Networks work?) - [55](https://eunmastudio.tistory.com/55)**

• 신경망이 입력 데이터를 처리하고 예측을 수행하는 과정을 실전 예제를 통해 알아봅니다.

4. **신경망의 학습 과정(How do Neural Networks learn?) - [55](https://eunmastudio.tistory.com/55)**

• 신경망이 데이터를 학습하는 과정과 오차를 줄이는 방법을 배웁니다.

5. **경사 하강법(Gradient Descent) - [56](https://eunmastudio.tistory.com/56)**

• 비용 함수(Cost Function)를 최소화하기 위한 경사 하강법(Gradient Descent)의 개념과 적용 방법을 학습합니다.

6. **확률적 경사 하강법(Stochastic Gradient Descent)**\- [56](https://eunmastudio.tistory.com/56)****

• 전체 데이터가 아닌 일부 데이터를 사용하여 학습하는 Stochastic Gradient Descent(SGD)의 개념과 장점을 이해합니다.

7. **역전파(Backpropagation)**\- [57](https://eunmastudio.tistory.com/57)****

• 신경망이 가중치를 업데이트하는 핵심 알고리즘인 역전파(Backpropagation)의 원리를 익히고, 이를 통해 모델을 최적화하는 방법을 배웁니다.

* * *

_**활성화 함수**_
------------

**활성화 함수란?**
------------

*   활성화 함수는 뉴런이 입력 값을 받아 출력을 결정할 때 적용하는 함수.
*   신경망이 단순한 선형 모델이 아니라 비선형적인 복잡한 패턴을 학습할 수 있도록 만들어 주는 핵심 요소!

### **왜 활성화 함수가 필요할까?**

*   신경망이 단순 선형 함수라면, 아무리 층을 많이 쌓아도 결과는 여전히 **선형 변환**일 뿐이야.
*   **비선형성(Non-linearity)** 을 추가해야 신경망이 복잡한 관계를 학습할 수 있어.
*   즉, **활성화 함수가 없으면 깊은 신경망이더라도 선형 회귀와 다를 바 없게 됨!**

* * *

**대표적인 활성화 함수 4가지**
-------------------

### **1\. 한계값 함수 (Threshold Function)**

**정의**

*   특정 임계값(threshold) 이상이면 1, 아니면 0을 출력하는 함수

![](https://blog.kakaocdn.net/dn/bjmdEC/btsMjKjOT2A/zRL9a0fLx0X5vgWPTEdaX0/img.png)

한계값 함수

​

**특징**

*   아주 단순한 **이진 분류(binary classification)** 에 사용
*   예/아니오 (0 또는 1) 형태로 결과를 출력
*   **단점:** 기울기가 없어서 가중치 업데이트가 어렵고, 작은 변화에도 너무 급격하게 변함

**사용 예시**

*   단순 퍼셉트론(Perceptron) 모델에서 사용
*   XOR 문제 해결 불가 → 다층 신경망(MLP)에서는 거의 사용하지 않음

* * *

### **2\. 시그모이드 함수 (Sigmoid Function)**

**정의**

*   출력 값이 **0과 1 사이**의 값이 되도록 변환하는 함수

![](https://blog.kakaocdn.net/dn/bdKxC3/btsMkAtIZvQ/AehTvgqPfFdEAV7aWBZBK1/img.png)

​

**특징**

*   **매끄러운 S자 곡선**을 그림
*   작은 입력값에서는 0에 가깝고, 큰 입력값에서는 1에 가까움
*   **확률 기반 예측에 유용** (예: 로지스틱 회귀에서 사용)

**장점**

*   확률값(0~1)을 출력하는 특성 덕분에 출력층에서 유용
*   로지스틱 회귀와 유사 → 이진 분류 문제에서 많이 사용됨

**단점**

*   큰 입력값에서는 기울기가 0에 가까워지는 Vanishing Gradient(기울기 소실) 문제 발생
*   값이 0에 가까운지, 1에 가까운지만 알려줄 뿐 중간값을 활용하기 어려움

**사용 예시**

*   **출력층에서 확률 값 예측 (이진 분류)**
*   로지스틱 회귀, 신경망의 마지막 층

* * *

### **3\. ReLU 함수 (Rectified Linear Unit)**

**정의**

*   입력이 0 이하이면 0, 0보다 크면 그대로 출력

![](https://blog.kakaocdn.net/dn/dqUK3I/btsMkcNAWfI/5eIfqnRrmn3ZvfqCRRJhbK/img.png)

​

**특징**

*   딥러닝에서 가장 **자주 사용되는 활성화 함수**
*   **음수 값을 0으로 변환**하여 불필요한 뉴런을 제거하는 효과
*   **기울기 소실 문제 해결** (Gradient Vanishing 해결)

**장점**

*   계산이 매우 빠름 → 학습 속도 향상
*   신경망이 깊어질수록 잘 동작함

**단점**

*   입력이 0보다 작으면 항상 0을 출력 → Dead Neuron(죽은 뉴런) 문제 발생
*   너무 큰 값에서는 기울기가 1로 고정되며 과적합 가능성 존재

**사용 예시**

*   **은닉층(Hidden Layer)에서 가장 많이 사용**
*   CNN(합성곱 신경망), RNN(순환 신경망) 등 거의 모든 딥러닝 모델

* * *

### **4\. 하이퍼볼릭 탄젠트 (Tanh) 함수**

**정의**

*   시그모이드 함수와 유사하지만, **출력 범위가 -1 ~ 1**

![](https://blog.kakaocdn.net/dn/b9Le4S/btsMlkxdcLf/3KkrH7ByiBQE5kgPEzNLH1/img.png)

​

**특징**

*   중심값이 0 → 학습이 더 안정적
*   시그모이드보다 기울기가 커서 **학습이 더 빠르게 진행됨**

**장점**

*   출력값이 -1 ~ 1 사이로 분포 → 데이터의 중심이 0이 되어 학습에 유리
*   시그모이드보다 기울기 소실 문제가 덜함

**단점**

*   여전히 Vanishing Gradient 문제 발생 가능
*   ReLU보다 계산량이 많음

**사용 예시**

*   RNN(순환 신경망)에서 많이 사용됨 (장기 의존성 문제 해결)
*   감성 분석(NLP) 같은 경우에서 유용

* * *

**활성화 함수 선택 기준**
----------------

### **어떤 활성화 함수를 언제 사용해야 할까?**

**용도** 

추천 활성화 함수

**이진 분류 (0 or 1)**

시그모이드 (출력층)

**다중 분류 (여러 개의 클래스 분류)**

소프트맥스 (Softmax, 출력층)

**은닉층에서 일반적으로 사용**

ReLU

**RNN (순환 신경망)에서 사용**

Tanh, LSTM의 경우 ReLU도 사용 가능

**기울기 소실 문제 해결 필요**

ReLU (대부분의 딥러닝 모델에서 기본 선택)

**활성화 함수 조합 예시**

*   **은닉층:** ReLU
*   **출력층(이진 분류):** 시그모이드
*   **출력층(다중 분류):** 소프트맥스

* * *

 **최종 요약**
----------

**활성화 함수란?**

신경망에서 **출력을 결정하는 비선형 함수**

모델이 복잡한 관계를 학습할 수 있도록 해줌

**주요 활성화 함수 4가지**

**한계값 함수:** 단순 이진 분류 (거의 사용 안 함)

**시그모이드:** 확률 출력이 필요할 때 (출력층)

**ReLU:** 은닉층에서 가장 많이 사용됨 (Dead Neuron 문제 주의)

**Tanh:** 시그모이드보다 개선된 형태, 주로 RNN에서 사용

**실제 사용 예시**

은닉층에서는 **ReLU** 사용이 기본

출력층에서 **이진 분류 → 시그모이드**, **다중 분류 → 소프트맥스**

* * *

(adsbygoogle = window.adsbygoogle || \[\]).push({}); if(window.observeAdsenseUnfilledState !== undefined){ observeAdsenseUnfilledState(); }

window.ReactionButtonType = 'reaction'; window.ReactionApiUrl = '//eunmastudio.tistory.com/reaction'; window.ReactionReqBody = { entryId: 54 }

공유하기

게시글 관리

**EunmaStudio**

#### '[\[STUDY\]](/category/%5BSTUDY%5D) > [\[공부\]](/category/%5BSTUDY%5D/%5B%EA%B3%B5%EB%B6%80%5D)' 카테고리의 다른 글

[\[딥러닝의 모든것\] - 딥러닝, 뉴런](/53)  (0)

2025.02.16
            